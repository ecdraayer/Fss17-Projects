CHOOSING K:
As we increase our k, the smoother our boundaries between our large clusters become. Increasing k also has the effect of causing smaller clusters within our data that are close to the larger clusters to disappear. A large k might have a large misclassification number for our training data, but might be better than a very small k. Small k values caused our boundaries between clusters to be very jagged, and caused us to have mini clusters within our large clusters. Small k had a small misclassification rate for our training data but would probably prove to be overfitted if we brought in new unknown data. Therefore we should try to aim for a k in-between the small and the large but this depends on our data. If the data has well defined clusters, then we should lean more towards a large k to guarantee our model is not over fitted and it will still be able to classify well. If our data has smaller clusters close to large cluster we should try for a smaller k to try and preserve these smaller clusters.

CHOOSING KERNEL:
From our results we can see that the linear kernel creates straight edges for our decision boundaries. This would be good for text data because the data can be linearly separated. 

The polynomial and rbf kernels allow us to create non linear decision boundaries. The polynomial kernel has a higher chance of misclassifying test data since there are very few scenarios in which data is separated by a polynomial boundary. But if it is, then it is better to use polynomial over rbf. However in general cases in which you know the separation is not linear then rbf will probably perform better in classifying test and training data over polynomial. 