CHOOSING K:
As we increase our k, the smoother our boundaries between our large clusters become. Increasing k also has the effect of causing smaller clusters within our data that are close to the larger clusters to disappear. A large k might have a large misclassification number for our training data, but might be better than a very small k. Small k values caused our boundaries between clusters to be very jagged, and caused us to have mini clusters within our large clusters. Small k had a small misclassification rate for our training data but would probably prove to be overfitted if we brought in new unknown data. Therefore we should try to aim for a k in-between the small and the large but this depends on our data. If the data has well defined clusters, then we should lean more towards a large k to guarantee our model is not over fitted and it will still be able to classify well. If our data has smaller clusters close to large cluster we should try for a smaller k to try and preserve these smaller clusters.

CHOOSING KERNEL:
